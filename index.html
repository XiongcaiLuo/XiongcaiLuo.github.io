<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>西瓜的木屋</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="西瓜的木屋">
<meta property="og:url" content="http://luoxc.com/index.html">
<meta property="og:site_name" content="西瓜的木屋">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="西瓜的木屋">
  
    <link rel="alternate" href="/atom.xml" title="西瓜的木屋" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="index.html" id="logo">西瓜的木屋</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="index.html" id="subtitle">记录关于机器学习、智能逻辑、人文社科方面的读书心得</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="index.html">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://luoxc.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-gan-basic" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="2018/01/06/gan-basic/" class="article-date">
  <time datetime="2018-01-06T14:50:04.000Z" itemprop="datePublished">2018-01-06</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="categories/机器的理智/">机器的理智</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="2018/01/06/gan-basic/">GAN的基本原理</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <hr>
<h2 id="GAN-简介"><a href="#GAN-简介" class="headerlink" title="GAN 简介"></a>GAN 简介</h2><h3 id="GAN的工作原理"><a href="#GAN的工作原理" class="headerlink" title="GAN的工作原理"></a>GAN的工作原理</h3><p>generator 和 discriminator相互博弈：</p>
<ul>
<li>discrimiator最大化真实样例与generator样例之间的差异</li>
<li>generator根据discriminator“反馈的指导信息”，更新参数，生成“更靠谱”的样例，减小与真实样例的差异。</li>
</ul>
<p><strong>Minimax Game:</strong><br>$$<br>    min_G\; max<em>D\; V(G, D)<br>$$<br><strong>在origin GAN中：</strong><br>$$<br>    V = E</em>{x\sim P<em>{data}}[logD(x)] + E</em>{x \sim P_G}[log(1-D(x))]<br>$$<br>一般而言，G是neural network, 它从一个先验分布$P<em>z$,生成x,上式写成：<br>$$<br>    V = E</em>{x\sim P<em>{data}}[logD(x)] + E</em>{z \sim P_z}[log(1-D(G(z)))]<br>$$</p>
<h3 id="GAN的应用示例"><a href="#GAN的应用示例" class="headerlink" title="GAN的应用示例"></a>GAN的应用示例</h3><p>目前，Tensorflow 1.4已经提供了一些gan的实现，在tf.contrib.gan中；另外，有很多开源的GAN的实现。（示例略，可以参加mnist上的各种实验和DCGAN、WGAN等生成的图片）</p>
<h2 id="GAN与ML"><a href="#GAN与ML" class="headerlink" title="GAN与ML"></a>GAN与ML</h2><h3 id="LR判别模型"><a href="#LR判别模型" class="headerlink" title="LR判别模型"></a>LR判别模型</h3><p>样本实例集合：$D = {(x^i, y^i)}<em>{i=1}^n$<br>利用最大似然(ML), 求解判别模型：$h</em>{\theta}(x) = \frac{1}{1+e^{-\theta^T x}} $<br>$$<br>\theta^<em> = arg\ max\ \frac{1}{n}\sum<em>{i=1}^n y^i log\ h</em>\theta(x^i) + (1-y^i)log(1-h<em>\theta(x^i)) \<br>=  arg\ max\ \frac{1}{n} \sum</em>{y^i=1}log\ h<em>{\theta}(x^i) + \sum</em>{y^j=0} log(1-h_\theta(x^j))\<br>=  arg\ max\ \frac{|D_1|}{n} \frac{1}{|D<em>1|} \sum</em>{D<em>1}log\ h</em>{\theta}(x^i) + \frac{|D_0|}{n} \frac{1}{|D<em>0|} \sum</em>{D<em>0}log\ h</em>{\theta}(x^j)\<br>= arg\ max\ P(y=1)E<em>{x\sim P(x|y=1)}[log h</em>\theta(x)] +  P(y=0)E<em>{x\sim P(x|y=0)}[log (1-h</em>\theta(x))]<br>$$<br>事实上，当假设空间$h<em>\theta(x)$有足够强的表征能力，（比如真实分布确实由LR模型生成，或者$h</em>\theta$是深层神经网络，可以表征任意函数）；通过求导，可以得到最优解为：<br>$$<br>    h^</em><em>\theta(x) = \frac{P(y=1)P(x|y=1)}{P(y=1)P(x|y=1) + P(y=0)P(x|y=0)} \<br>    = \frac{P(x, y=1)}{P(x) } = P(y=1 | x)<br>$$<br>(额，貌似推理了一句废话，不过这个公式正说明，当我们采用ML或者cross entropy的时候，最优解正是后验概率（条件概率），前提是$h</em>\theta(x)$有足够强的表征能力。推导这个式子，也可以和后面推导$D^*$相互验证)<br>观察式子：</p>
<ul>
<li>$x^i $是正例， $h_\theta(x^i)$尽可能大，接近1</li>
<li>$x^j $是负例， $h_\theta(x^j)$尽可能小，接近0</li>
<li>或者添加负号，可以从极小化negative log loss的角度考虑。</li>
</ul>
<p>对于GAN而言，某种程度上，D是h:</p>
<ul>
<li>$x^i \sim P_{data}$, 是<strong>“正例”</strong>，判别器D应使得$D(x^i)$尽可能大，接近1，即极大化$log(D(x^i))$</li>
<li>$x^j \sim P_{G}$, 是<strong>“负例”</strong>，判别器D应使得$D(x^j)$尽可能小，接近0, 即极大化 $log(1-D(x^j))$</li>
<li>类似地，忽略先验概率，V函数定义为$$V = E<em>{x\sim P</em>{data} }[logD(x)] + E<em>{x \sim P</em>{G}}[log(1-D(x))] = E<em>{x\sim P</em>{data} }[logD(x)] + E_{z \sim P_z}[log(1-D(G(z)))] $$,  而$D^* = max_D\; V(G,D)$; （这里忽略$P(y)$,两类先验概率相等，正对应后面训练D时，进行相等数量的sample）</li>
</ul>
<p><strong>事实上，训练判别器D的过程，正是使用ML求解二分类问题</strong>：</p>
<ul>
<li>sample $x^1, x^2 \cdots x^n$ from $P_{data}(x)$, 作为正例</li>
<li>sample  $\tilde{x^1}, \tilde{x^2} \cdots \tilde{x^n}$ from $P_{G}(x)$ （实际是sample z）， 作为负例</li>
<li>利用最大似然求解$V = \frac{1}{n}\sum<em>{i=1}^n log\ D(x^i) + \frac{1}{n}\sum</em>{i=1}^n log(1-D(\tilde{x}^i)) $</li>
</ul>
<p>(同样的思想，有“NCE”， “negative sampling”)</p>
<h3 id="ori-GAN和ML分别衡量不同的divergency"><a href="#ori-GAN和ML分别衡量不同的divergency" class="headerlink" title="ori-GAN和ML分别衡量不同的divergency"></a><strong>ori-GAN和ML分别衡量不同的divergency</strong></h3><ul>
<li>ML 衡量生成模型与真实概率分布的KL距离</li>
<li>ori-GAN衡量JS距离</li>
</ul>
<h4 id="ML-and-KL-divergency"><a href="#ML-and-KL-divergency" class="headerlink" title="ML and KL divergency"></a>ML and KL divergency</h4><p>未知的真实分布：$P<em>{data}(x)$<br>样本实例集：$D = {x^i }</em>{i=1}^n$， 采样自$P_{data}(x)$<br>假设空间中的生成模型：$P<em>G(x;\theta)$来模拟$P</em>{data}(x)$<br>根据ML原则：</p>
<p>$$ \theta^* = arg\; max<em>{\theta} \;  \prod</em>{i=1}^n P<em>G(x^i;\theta) \ = arg\; max</em>{\theta} \; \frac{1}{n}\sum_{i=1}^n log\ P<em>G(x^i; \theta) \ \approx arg\; max</em>{\theta} \; E<em>{x\sim P</em>{data} [log  P<em>G(x; \theta)]} \ = arg\; max</em>{\theta} \; \int<em>x P</em>{data}(x)logP_G(x;\theta)dx - \int<em>x P</em>{data}(x)logP<em>{data}(x)dx \ = arg\; min</em>{\theta}\; KL(P_{data}(x)\ ||\ P_G(x;\theta) ) $$</p>
<p>所以，对生成模型采用ML原则，实际最小化KL距离。</p>
<h4 id="origin-GAN-and-JS-Divergency"><a href="#origin-GAN-and-JS-Divergency" class="headerlink" title="origin-GAN and JS Divergency"></a>origin-GAN and JS Divergency</h4><ol>
<li><strong>给定G, 求解$D^<em> = max_D V(G,D)$；此时$V(G,D</em>)$衡量$ P_{data}, P_G$之间JS divergency</strong></li>
</ol>
<p>$$<br>max_D\ V(G,D) =max_D\ \int<em>x[P</em>{data}(x) log D(x) + P<em>G(x)log(1-D(x))] dx \Rightarrow \  D^*(x) = \frac{P</em>{data}(x)}{P_{data}(x) + P_G(x)}<br>$$</p>
<p>类比前面的LR的最优解$h^<em>_\theta(x)$，$D^</em>$表示在先验概率相等的前提下，后验概率$P(x来自于真实data|x)$</p>
<p>此时，<br>$$<br>V(G,D^*) = \int<em>x[P</em>{data}(x) log \frac{P<em>{data}(x)}{P</em>{data}(x) + P_G(x)} + P<em>G(x)log(1-\frac{P</em>{data}(x)}{P_{data}(x) + P_G(x)})] dx \ =  \int<em>x[P</em>{data}(x) log \frac{P<em>{data}(x)}{(P</em>{data}(x) + P_G(x))/2} + P<em>G(x)log\frac{P</em>{G}(x)}{(P_{data}(x) + P<em>G(x))/2}] dx - 2log2\ = KL(P</em>{data}(x) || \frac{P_{data}(x) + P<em>G(x)}{2}) + KL(P</em>{G}(x) || \frac{P_{data}(x) + P<em>G(x)}{2}) - 2log2\ = 2JSD(P</em>{data}, P_G) - 2log2<br>$$</p>
<ol>
<li><strong>求解G使得 $G^<em> = min_G V(G, D^</em>) $</strong></li>
</ol>
<h4 id="GAN的训练过程"><a href="#GAN的训练过程" class="headerlink" title="GAN的训练过程"></a>GAN的训练过程</h4><p><img src="/2018/01/06/gan-basic/train_gan.png" alt="train_gan"><br>（<strong>GAN的完整训练过程</strong>。图片来自于“李宏毅 深度学习”课程）<br>(Ian Goodfellow, 在原始论文中改训练G为$-log(D(G(z)))$, 这个训练目标是从收敛的角度来考虑的)</p>
<h4 id="GAN的特别之处在哪里？"><a href="#GAN的特别之处在哪里？" class="headerlink" title="GAN的特别之处在哪里？"></a>GAN的特别之处在哪里？</h4><ul>
<li>ML的训练会可能很麻烦：采用显式的概率分布（模型空间可能不够准确）；采用隐式的概率推断会涉及比较复杂的方法</li>
<li>GAN提供了另外一种方案，它直接利用BP来优化概率分布距离的方法：求解generator和discriminator的minimax博弈。generator和discriminator都采用neural network，有足够强大的表征能力（给一个表征能力的实验）。</li>
<li>GAN提供了一个框架，可以将ML纳入进来，甚至可以按需设计其它的函数V（参见fGAN）</li>
</ul>
<p><strong>那么能否在GAN的框架下，将ML与原始GAN统一起来？能否使用其它的概率距离度量？</strong></p>
<h2 id="fGAN-GAN的统一框架"><a href="#fGAN-GAN的统一框架" class="headerlink" title="fGAN: GAN的统一框架"></a>fGAN: GAN的统一框架</h2><h3 id="f-divergency"><a href="#f-divergency" class="headerlink" title="f-divergency"></a>f-divergency</h3><p><strong>定义：</strong><br>$$D_f(P || Q) = \int_x q(x) f(\frac{p(x)}{q(x)})dx,\ 且f是convex, f(1)=0$$ </p>
<p><strong>例子：</strong></p>
<ul>
<li>$f = xlogx, D_f(P||Q) = KL(P||Q)$</li>
<li>$f = -logx, D_f(P||Q) = revserse\ KL(P||Q)$</li>
<li><p>$f=ulogu-(u+1)log(u+1), D_f(P||Q) =2JS(P||Q) - 2log2$</p>
</li>
<li><p>f=ulogu-(u+1)log(u+1), D_f(P||Q) =2JS(P||Q) - 2log2$</p>
</li>
</ul>
<h3 id="Fenchel-Conjugate"><a href="#Fenchel-Conjugate" class="headerlink" title="Fenchel Conjugate:"></a><strong>Fenchel Conjugate:</strong></h3><p>$$f^<em>(t) = \max\limits<em>{x \in Dom(f)} {xt-f(x)}$$<br>$$f(x) = \max\limits</em>{t \in Dom(f^</em>)} {xt-f^<em>(t)}$$<br>（注：$f^</em>(t)$也是convex, 它是一系列仿射函数的max）</p>
<p><strong>事实上，Fenchel Conjugate定义了“斜率（梯度）到截距”的一种映射</strong><br>当固定$t $,  $f^<em>(t) = \max\limits_{x \in Dom(f)} {xt-f(x)}$, 通过对x求导得到：$$t=  f’(x), \ f^</em>(t) = xf’(x) - f(x)$$<br>上式可以看做参数方程的形式定义了$f^<em>$，它的几何意义：对任意的x，作f(x)的切线，斜率为t, 与y的截距的负数为$f^</em>(t)$; 它定义了斜率和负截距的映射关系。</p>
<p><strong>重要的是，上述映射关系的对偶性质！</strong></p>
<h3 id="与GAN的联系"><a href="#与GAN的联系" class="headerlink" title="与GAN的联系"></a>与GAN的联系</h3><p>$$<br>    D_f(P||Q) = \int_x q(x) f(\frac{p(x)}{q(x)})dx\<br>    = \int<em>x q(x) \max\limits</em>{t\in dom(f^<em>)} { t\frac{p(x)}{q(x)} - f^</em>(t)} dx\<br>    \ge  \max\limits_{t\in dom(f^<em>)}  \int_x p(x)t dx -  \int_x q(x)f^</em>(t) dx<br>$$</p>
<p>这里，令D(x) = t, 上式的下界可以写作：<br>$$<br>     \max\limits_{D}  \int_x p(x)D(x)dx -  \int<em>x q(x)f^*(D(x)) dx \<br>     =  \max\limits</em>{D} { E<em>{x\sim p}[D(x)] - E</em>{x\sim q}[f^<em>(D(x))] }<br>$$<br>(事实上，如果D(x)表征能力足够强，最优解为$D^</em>(x) = f’(\frac{p(x)}{q(x)})$,但是这个无法直接求解 )<br>对于GAN而言：<br>$$<br>    D<em>f(P</em>{data}||P<em>{G})  \approx \max\limits</em>{D} { E<em>{x\sim P</em>{data}}[D(x)] - E_{x\sim P<em>G}[f^<em>(D(x))] }<br>$$<br>写成minimax形式：<br>$$ G^</em> = \arg \min\limits</em>{G}\max\limits_{D} V(G,D)$$</p>
<p><strong>按需挑选不同的f-divergency:</strong><br><img src="/2018/01/06/gan-basic/fgan_1.png" alt="fgan_1_"><br><img src="/2018/01/06/gan-basic/fgan_2.png" alt="fgan_2_"><br>(<strong>不同的f-divergency对应的GAN</strong>, 图片来自于论文 f-GAN)</p>
<h2 id="WGAN：解决收敛性问题"><a href="#WGAN：解决收敛性问题" class="headerlink" title="WGAN：解决收敛性问题"></a>WGAN：解决收敛性问题</h2><h3 id="origin-GAN面临的收敛问题"><a href="#origin-GAN面临的收敛问题" class="headerlink" title="origin-GAN面临的收敛问题"></a>origin-GAN面临的收敛问题</h3><p><strong>理想情况：D 指导$P<em>G$往真实分布$P</em>{data}$(dashed)运动</strong><br><img src="/2018/01/06/gan-basic/wgan_1.png" alt="fgan_1_"><br><strong>实际情况：D训练越好，完美区分，梯度消失，无指导能力</strong><br><img src="/2018/01/06/gan-basic/wgan_2.png" alt="fgan_2_"></p>
<p>考虑”parallel lines distribution”, 二维分布$P_{data}: (0, Z), 其中Z \sim U[0,1]$, $P_G: (\theta, Z)$:</p>
<ul>
<li>$JS(P_{data}, P_G) = |\theta|$</li>
<li>$KL(P_{data} || P_G) = KL(P<em>G || P</em>{data} ) = + \infty (\theta \ne 0), 0(\theta=0)$</li>
<li>Discriminator D往往能将$P_{data}, P_G$完美分开</li>
<li>如上图所示，在D看来，$d<em>0, d</em>{50}$的JSD都是log2; D没有动力，让$P_G$往“期望的方向”移动，会导致收敛问题</li>
<li>事实上，像生物进化一样，进化（比如眼睛）往往不是一蹴而就的；应该有更合适的度量方式，使得$P<em>G$向$P</em>{data}$“靠拢”（虽然此时JSD看来，generator并没有改善）</li>
</ul>
<h3 id="Earth-Mover’s-Distance"><a href="#Earth-Mover’s-Distance" class="headerlink" title="Earth Mover’s Distance"></a>Earth Mover’s Distance</h3><p><strong>定义：</strong>对于概率分布P,Q，average distance of a plan $\gamma$:<br>$$ B(\gamma) = \sum\limits_{x_p, x_q} \gamma(x_p, x_q) ||x_p, x<em>q|| $$<br>Earth Mover’s Distance:<br>$$ W(P,Q) = \min\limits</em>{\gamma \in \Pi} B(\gamma)$$<br>示意图如下：<br>本质上，$\gamma$就是一个联合概率，它的边缘分布分别为$P, Q$<br><img src="/2018/01/06/gan-basic/earth_mover.png" alt="earth_mover"><br>（<strong>Moving Plan</strong>, 图片来自于“李宏毅 深度学习”）<br>在上面的<strong>parallel line distribution</strong>例子里,</p>
<ul>
<li>$W(P_{data}, P_G) = |\theta|$</li>
</ul>
<h3 id="WGAN"><a href="#WGAN" class="headerlink" title="WGAN"></a>WGAN</h3><p>论文中证明，当我们采用Earth Mover’s Distance来度量$P_{data}, P<em>G$距离，相应的GAN形式如下(Kantorovich-Rubinstein duality, 来自论文“Optimal Transport: Old and New”)：<br>$$<br>    W(P</em>{data}, P<em>G) = \max\limits</em>{D \in 1-lipschitz} { E<em>{x\sim P</em>{data}}[D(x)] -  E<em>{x\sim P</em>{G}}[D(x)] }<br>$$<br>其中，<strong>1-lipschitz </strong>是指：<br>$$ ||D(x_1) - D(x_2)|| \le ||x_1 - x_2|| $$<br>该条件的限制，防止了D(x)的变化过于剧烈。这里，整个优化目标有点“返璞归真”的意思了。<br><strong>WGAN的论文中，使用weight-clipping近似1-lipschitz 条件</strong>：</p>
<ul>
<li>权重$|w| &gt; c \Rightarrow |w|=c$</li>
<li>实际使用的是<strong>k-lipschitz</strong><br><img src="/2018/01/06/gan-basic/wgan_3.png" alt="wgan_3_"></li>
<li>可以看到origin GAN 会存在梯度消失，无法有效指导$P_G$的方向</li>
<li>WGAN可以提供有效信息。</li>
<li>$W(P_{data}, P_G)$的值可以作为训练好坏的参考</li>
</ul>
<p><strong>improved WGAN</strong></p>
<p>将WGAN的1-lipschitz条件以惩罚项的形式引入：</p>
<p>​    $$W(P_{data}, P<em>G) = \max\limits</em>{D} { E<em>{x\sim P</em>{data}}[D(x)] -  E<em>{x\sim P</em>{G}}[D(x)] } - \lambda E<em>{x\sim P</em>{penalty}}[(||\nabla_x D(x)||-1)^2]$$</p>
<p>$P<em>{penalty}$的生成：对于$x \sim P</em>{data}, \tilde{x} \sim P<em>G$, 计算$x，\tilde{x}$之间的随机点，作为$x’ \sim P</em>{penalty}$</p>
<h3 id="GAN的家族"><a href="#GAN的家族" class="headerlink" title="GAN的家族"></a>GAN的家族</h3><table>
<thead>
<tr>
<th style="text-align:left">Modify the optimization of GAN</th>
<th style="text-align:right">Different structure from the original GAN</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">fGAN</td>
<td style="text-align:right">Conditional GAN</td>
</tr>
<tr>
<td style="text-align:left">WGAN</td>
<td style="text-align:right">Semi-GAN</td>
</tr>
<tr>
<td style="text-align:left">Least-square GAN</td>
<td style="text-align:right">InfoGAN</td>
</tr>
<tr>
<td style="text-align:left">Loss Sensitive GAN</td>
<td style="text-align:right">BiGAN</td>
</tr>
<tr>
<td style="text-align:left">Energy-based GAN</td>
<td style="text-align:right">Cycle GAN</td>
</tr>
<tr>
<td style="text-align:left">Boundary-Seeking GAN</td>
<td style="text-align:right">IRGAN</td>
</tr>
<tr>
<td style="text-align:left">Unroll GAN</td>
<td style="text-align:right">VAE GAN</td>
</tr>
<tr>
<td style="text-align:left">…</td>
<td style="text-align:right">…</td>
</tr>
</tbody>
</table>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://luoxc.com/2018/01/06/gan-basic/" data-id="cjc3i7unu00053y5dh6iwz5cp" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-auto-encoder" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="2017/12/31/auto-encoder/" class="article-date">
  <time datetime="2017-12-31T02:48:00.000Z" itemprop="datePublished">2017-12-31</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="categories/机器的理智/">机器的理智</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="2017/12/31/auto-encoder/">auto_encoder</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://luoxc.com/2017/12/31/auto-encoder/" data-id="cjc3i7unh00033y5diyonbadi" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-optimization" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="2017/12/31/optimization/" class="article-date">
  <time datetime="2017-12-30T18:49:46.000Z" itemprop="datePublished">2017-12-31</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="categories/机器的理智/">机器的理智</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="2017/12/31/optimization/">最优化方法</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>@(math)[最优化, 牛顿法, 梯度下降法, 拟牛顿法, 数值计算]</p>
<hr>
<h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p><strong>核心思想：负梯度方向是函数值下降最快的方向</strong><br>$$x_{i+1} = x_i - \eta f^{(1)}(x_i)$$<br>其中，$\eta &gt; 0$是学习率, 需要设置和调整。</p>
<h3 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h3><p><strong>核心思想：将函数$f(x)$进行泰勒展开，得到二阶多项式</strong>,如第$i$次迭代时：<br>$$f(x) = f(x_i) + \frac{f^{(1)}(x_i)}{1!}(x - x_i) + \frac{f^{(2)}(x_i)}{2!}(x-x_i)^2 + o((x-x_i)^2)$$<br>当$f(x)$取极值时，有$f^{(1)}(x) = 0$, 因此对上式求导，并舍弃高阶项，得到：<br>$$0 = f^{(1)}(x_i) +f^{(2)}(x_i)(x-x_i) $$<br>$$x = x_i - \frac{f^{(1)}(x_i)}{f^{(2)}(x<em>i)}$$<br>对于多元函数，相应的矩阵迭代形式如下：<br>$$x</em>{i+1} = x<em>i - H</em>{x<em>i}^{-1}\nabla</em>{x_i} f$$<br>其中$H$是海塞矩阵<br><strong>特点：</strong></p>
<ul>
<li>对于二次函数，或者比较接近二次的函数或者在极小值点附近，牛顿法比梯度下降法收敛的更快</li>
<li>对于一般函数，牛顿法可能导致计算发散不收敛。改进：<strong>阻尼牛顿法</strong>，在搜索方向$- H_{x<em>i}^{-1}\nabla</em>{x<em>i} f$ 上搜索合适的步长$\lambda$：$- \lambda H</em>{x<em>i}^{-1}\nabla</em>{x_i} f$ </li>
<li>对于高维特征，$H^{-1}$的计算代价将非常高昂; 可以令$-H^{-1}_k g_k = p<em>k$, 通过求解$H</em>{k}p_k = -g_k$ 得到步长$p_k$, 但是每次计算$H_k$代价仍然非常高</li>
</ul>
<p><strong>H提供的信息</strong><br>二阶导数是对曲率的衡量。对于步长为$\epsilon$,</p>
<ul>
<li>H为0：代价函数下降约$\epsilon$</li>
<li>H为负：代价函数下降大于$\epsilon$</li>
<li>H为正：代价函数下降小于$\epsilon$，甚至有可能增长<br><img src="/2017/12/31/optimization/hess.png" alt="hess"></li>
</ul>
<p><img src="/2017/12/31/optimization/eq1.png" alt="eq1"><br>当步长为$\epsilon$时，迭代<br><img src="/2017/12/31/optimization/eq2.png" alt="eq2"><br>此时，若$H$正定矩阵，那么最优步长<br><img src="/2017/12/31/optimization/eq3.png" alt="eq3"><br>最坏情况下，g与H的最大特征向量方向平行，此时步长为$\frac{1}{\lambda_{max}}$</p>
<p><strong>局部极值测试</strong></p>
<ul>
<li>$f’(x) = 0, H(x) &gt; 0$: 局部极小值</li>
<li>$f’(x) = 0, H(x) &lt; 0$: 局部极大值</li>
<li>$f’(x) = 0, H(x) = 0$: 不确定</li>
</ul>
<p><strong>条件数</strong><br>对于矩阵$A \in R^{n\times n}$, 若有特征值，那么定义条件数：<br>$$cond(A) = |\frac{\lambda<em>{max}}{\lambda</em>{min}} |$$<br>$H$是对称矩阵，一定有n个特征值(包含重数)。</p>
<ul>
<li>当条件数过大时，矩阵求逆会放大误差<ul>
<li>本身求逆不稳定</li>
<li>$H^{-1}g$会放大$g$的误差。</li>
</ul>
</li>
</ul>
<h3 id="拟牛顿法"><a href="#拟牛顿法" class="headerlink" title="拟牛顿法"></a>拟牛顿法</h3><h4 id="DFP算法"><a href="#DFP算法" class="headerlink" title="DFP算法"></a>DFP算法</h4><p>牛顿法中海塞矩阵的计算代价高昂，考虑用一个n阶矩阵$G_k$去近似代替$H_k^{-1} = H^{-1}(x<em>k)$<br>我们将f在点$x</em>{k+1}$展开：<br>$$<br>    f(x) = f(x<em>{k+1}) + g</em>{k+1}^T x<em>k + \frac{1}{2}(x - x</em>{k+1})^T H<em>{k+1}(x - x</em>{k+1}) \<br>    \nabla f(x) = g<em>{k+1} + H</em>{k+1}(x - x<em>{k+1} ) \<br>$$<br>这里，取$x = x</em>{k}$, 则<br>$$<br>    g<em>k =  g</em>{k+1} + H_{k+1}(x<em>k - x</em>{k+1} )<br>$$<br>令$y<em>k = g</em>{k+1} - g_k, \delta<em>k = x</em>{k+1} - x<em>k$, 则：<br>$$<br>    H</em>{k+1} \delta_k = y<em>k, 或\<br>    H</em>{k+1}^{-1} y_k = \delta_k<br>$$<br>称之为<strong>拟牛顿条件</strong>,给出了下面的迭代依据：</p>
<ul>
<li>使用正定矩阵$G<em>k$代替$H</em>{k}^{-1}$，且迭代求解$G_{k+1}$</li>
</ul>
<p>令$G_{k+1} = G_k + \Delta G = G_k + P_k + Q<em>k$ , 则$G</em>{k+1}y_k = G_k y_k + P_k y_k + Q_k y_k$, 若要满足<strong>拟牛顿条件</strong>，只需：<br>$$<br>     P_k y_k = \delta_k\<br>     Q_k y_k = -G_k y_k<br>$$<br>只需：<br>$$<br>\delta_k = \delta_k \frac{\delta_k^T y_k}{\delta_k^T y_k} =\frac{\delta_k \delta_k^T }{\delta_k^T y_k} y_k \<br>令P_k = \frac{\delta_k \delta_k^T }{\delta_k^T y_k} 即可\<br>同理: Q_k = -\frac{G_k y_k y_k^T G_k}{y_k^T G_k y_k}<br>$$<br><strong>DFP算法伪代码：</strong><br><img src="/2017/12/31/optimization/DFP.png" alt="DFP"></p>
<h4 id="BFGS算法"><a href="#BFGS算法" class="headerlink" title="BFGS算法"></a>BFGS算法</h4><p>原理同DFP，但是直接使用$B_k$来模拟$H_k$而不是$H<em>k^{-1}$<br>$$<br>B</em>{k+1} = B_k + \frac{y_k y_k^T}{y_k^T \delta_k} - \frac{B_k \delta_k \delta_k^T B_k}{\delta_k^T B_k \delta_k}<br>$$<br><strong>BFGS算法伪代码：</strong><br><img src="/2017/12/31/optimization/BFGS.png" alt="BFGS"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://luoxc.com/2017/12/31/optimization/" data-id="cjc3i7up7000m3y5decb1v9v6" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-unbias-estimator" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="2017/12/31/unbias-estimator/" class="article-date">
  <time datetime="2017-12-30T18:43:18.000Z" itemprop="datePublished">2017-12-31</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="categories/机器的理智/">机器的理智</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="2017/12/31/unbias-estimator/">无偏估计</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><strong>Bessel’s correction</strong></p>
<ul>
<li>计算协方差的时候并不是除以N, 那是因为每一列的和为0，因此自由度为N-1<ul>
<li>解释bias来源：<a href="https://en.wikipedia.org/wiki/Bessel%27s_correction#Source_of_bias" target="_blank" rel="external">https://en.wikipedia.org/wiki/Bessel%27s_correction#Source_of_bias</a><ul>
<li>本质上就是样本均值(sample mean)与总体均值(population mean)不同。此时，样本方差总是比真实方差小</li>
</ul>
</li>
</ul>
</li>
<li>修正后，方差是无偏的，但标准差是有偏的。因为sqrt是上凸函数<ul>
<li>标准差无偏估计：<a href="https://en.wikipedia.org/wiki/Unbiased_estimation_of_standard_deviation" target="_blank" rel="external">https://en.wikipedia.org/wiki/Unbiased_estimation_of_standard_deviation</a></li>
<li>自由度：能独立变化的变量个数</li>
</ul>
</li>
<li>当真实均值给定(而不是从样本中计算)时候，除以N</li>
</ul>
<p><strong>sample var, population var</strong></p>
<ul>
<li><p>$\mu$: population mean（总体均值）</p>
</li>
<li><p>$\delta^2$: population variance</p>
</li>
<li><p>$\bar{x}$: sample mean(样本均值)</p>
</li>
<li><p>$s_n^2$: sample variance（n个样本方差）</p>
<p>可以将$\bar{x}, s_n^2$看做随机变量，它们是对$\mu, \delta^2$的估计。</p>
<p>对于$\bar{x}$有：$$ E[\bar{x}] = \mu, 无偏估计$$<br>$$ E[(\bar{x} - \mu)^2] = \frac{\delta^2}{n}$$<br>对于$s_n^2$有：<br>$$ E[s<em>n^2] = E[\frac{1}{n} \sum</em>{i=1}^n(x<em>i - \bar{x})^2]  = E[\frac{1}{n} \sum</em>{i=1}^n(x<em>i - \mu + \mu - \bar{x})^2] \ = E[\frac{1}{n} \sum</em>{i=1}^n(x_i - \mu)^2 + (\mu - \bar{x})^2 + 2(x<em>i-\mu)(\mu - \bar{x})] \ = \frac{1}{n} \sum</em>{i=1}^nE[(x_i-\mu)^2] + E[(\mu - \bar{x})^2 + 2(\bar{x}-\mu)(\mu - \bar{x})] \ = \delta^2 - E[(\mu- \bar{x})^2] = \delta^2 - \frac{\delta^2}{n}  = \frac{n-1}{n}\delta^2$$</p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://luoxc.com/2017/12/31/unbias-estimator/" data-id="cjc3i7upv000w3y5dar7qywri" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-norm-dist" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="2017/12/31/norm-dist/" class="article-date">
  <time datetime="2017-12-30T18:40:59.000Z" itemprop="datePublished">2017-12-31</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="categories/机器的理智/">机器的理智</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="2017/12/31/norm-dist/">正态分布</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="单变量正态分布"><a href="#单变量正态分布" class="headerlink" title="单变量正态分布"></a>单变量正态分布</h3><p><strong>定义：</strong> $p(x) = \frac{1}{\sqrt{2\pi}\delta}e^{-\frac{1}{2}(\frac{x-\mu}{\delta})^2}$</p>
<ul>
<li>$E(x) = \mu$, $D(x) = \delta^2$</li>
<li>$y = ax+b$ 是正态分布，其中a,b是常数<ul>
<li>$E(y) = a\mu+b,  D(y) = a^2\delta^2$</li>
</ul>
</li>
<li>$y = \sum_{i=1}^{n}x_i$, 且$x_i$都是正态分布时， $y$是正态分布？<ul>
<li>$E(y) = \sum_{i=1}^{n}\mu<em>i,  D(y) =\sum</em>{i=1}^{n}\delta_i^2 $</li>
</ul>
</li>
</ul>
<h3 id="多元正态分布"><a href="#多元正态分布" class="headerlink" title="多元正态分布"></a>多元正态分布</h3><p><strong>定义：</strong>当随机向量$X=\mu + B\epsilon$满足: $u$是n维常向量，$B$是n*m维矩阵， $\epsilon = [\epsilon_1, \epsilon_2…\epsilon_m]$的分量都是相互独立的标准正态分布时，称$X$满足多元正态分布。</p>
<ul>
<li>即$X$的每一维都能分解成某一共同的<strong>m个相互独立的标准正态分布的线性组合</strong></li>
<li>$E(X) = \mu, \Sigma = BB^T$</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://luoxc.com/2017/12/31/norm-dist/" data-id="cjc3i7up4000k3y5dnn4cpwjz" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-tf-tutorial" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="2017/12/31/tf-tutorial/" class="article-date">
  <time datetime="2017-12-30T17:13:22.000Z" itemprop="datePublished">2017-12-31</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="categories/机器的理智/">机器的理智</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="2017/12/31/tf-tutorial/">Tensorflow简明导论</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>该文主要来自于TensorFlow白皮书，摘录了重点，以备不时之需。</p>
<hr>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul>
<li><a href="https://www.tensorflow.org/" target="_blank" rel="external">https://www.tensorflow.org/</a></li>
<li>TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems</li>
</ul>
<h3 id="Basic-concepts"><a href="#Basic-concepts" class="headerlink" title="Basic concepts"></a>Basic concepts</h3><p><strong>Operations</strong>: abstract computation(add, matrix multiply)</p>
<ul>
<li>attributes: </li>
<li>inferred at graph-construction time</li>
</ul>
<p><strong>kernel</strong>: particular implementation of an operation on a particular type of device(GPU, CPU)</p>
<p><strong>Tensors</strong></p>
<ul>
<li>typed, multi-dimensional array</li>
<li>input or output of operations</li>
<li>reference counted, and deallocated while no references remain.</li>
</ul>
<p><strong>Variable</strong>: </p>
<ul>
<li><strong>special operations</strong>, return a handle to a persistent mutable tensor</li>
<li>“<strong>parameters</strong>“, survive across the whole session; updated as part of “Run”</li>
<li>should call “initializer” in a session.</li>
</ul>
<p><strong>placeholder</strong></p>
<ul>
<li>“inputs” of a machine learning problem</li>
<li>feed_dict of “Run”</li>
</ul>
<p><strong>Graph</strong></p>
<ul>
<li>represented the whole computation as a dataflow graph</li>
<li>node: Operation objects(add, multiply, etc.)</li>
<li>edge:  tensor</li>
</ul>
<p><strong>Session</strong>: </p>
<ul>
<li>used to enable Client to interact with TF sys, manage the “Graph” computation</li>
<li>“Extend” method: augment the graph managed by the session</li>
<li>“Run” method:  fetch names to be computed; feed tensors as input.</li>
<li>run the whole graph or a few distinct subgraphs once</li>
</ul>
<p><strong>角色分工</strong></p>
<ul>
<li><strong>Client</strong>: the user side with TF codes， use Session to interact with “master”</li>
<li><strong>Master</strong>: TF system role for scheduling</li>
<li><strong>Worker</strong>: managed by master, responsible for managing “Devices” and run the actual “Tasks”.</li>
<li><strong>Job and task:</strong> 实际的一次任务执行称为一个job，job由多个task(分布式环境下)构成，由workder实际执行；单机时，client, master, worker 在一个os process环境下 。</li>
<li><strong>Devices:</strong> 实际的运算实体（CPU， GPU等）<ul>
<li>device type and name</li>
<li>device name example:  “ /job:localhost/device:cpu:0”</li>
<li>device name example:  “/job:worker/task:17/device:gpu:3”<br><img src="/2017/12/31/tf-tutorial/single_distribute_structure.png" alt="single_distribute_structure"></li>
</ul>
</li>
</ul>
<h3 id="Execution"><a href="#Execution" class="headerlink" title="Execution"></a>Execution</h3><h4 id="single-device-execution"><a href="#single-device-execution" class="headerlink" title="single-device execution"></a>single-device execution</h4><ul>
<li>依据Graph的依赖关系执行</li>
<li>每个node维护一个dependency counter; 当counter为0时，进入ready queue等待执行</li>
<li>node s 执行完毕，将所有依赖s的下游node的counter减1</li>
</ul>
<h4 id="Multi-device-execution"><a href="#Multi-device-execution" class="headerlink" title="Multi-device execution"></a>Multi-device execution</h4><p><strong>Node placement</strong></p>
<ul>
<li>核心：基于cost model进行simulation, 将node放置在合适的device上</li>
<li>cost model:<ul>
<li>input,output  tensors’ sizes</li>
<li>computation time</li>
</ul>
</li>
<li>simulation:<ul>
<li>使用贪心策略，依据Graph dependency，依次决定nodes的放置。</li>
<li>对于Node s，考虑s所有可行的devices, 依据下面两条决定：<ul>
<li>comunication time (if has): 可能的传输时间，如跨设备或分布式</li>
<li>cost model<br><strong>Cross-device comunication</strong></li>
</ul>
</li>
</ul>
</li>
<li>vertex-centric partiton.</li>
<li>在每个device上增加特殊节点：”Send”, “Receive”<ul>
<li>隔离：所有跨设备的通信交由Send和Receive来处理，包括同步等。</li>
<li>节省内存：在同一设备上多个节点依赖同一个tensor,只需要Receive管理一份，大大减少传输量和内存占用。</li>
</ul>
</li>
</ul>
<p><img src="/2017/12/31/tf-tutorial/multi_device_commu.png" alt="multi_device_commu"></p>
<h4 id="distributed-execution"><a href="#distributed-execution" class="headerlink" title="distributed execution"></a>distributed execution</h4><p>大部分执行机制同“Multi-device execution”</p>
<h4 id="Fault-Tolerance"><a href="#Fault-Tolerance" class="headerlink" title="Fault Tolerance"></a>Fault Tolerance</h4><p><strong>出错检测</strong></p>
<ul>
<li>Send, Recieve 通信报错</li>
<li>心跳机制检测</li>
</ul>
<p><strong>check-point 机制</strong></p>
<ul>
<li>Variable: 连接到Save node和Restore node<ul>
<li>每隔一定周期，Save node对variable进行持久化</li>
<li>恢复时，只在restart的第一次迭代时，将值赋给Variable</li>
</ul>
</li>
</ul>
<h3 id="Gradient-Computation"><a href="#Gradient-Computation" class="headerlink" title="Gradient Computation"></a>Gradient Computation</h3><p>TF提供自动的符号梯度计算</p>
<ul>
<li>[db,dW,dx] = tf.gradients(C, [b,W,x])<ul>
<li>extending the computation graph</li>
<li>backtracks from target C to source I, add “partial gradient node” to the Graph</li>
<li>梯度的计算不仅依赖”partial gradient nodes”,还依赖前向过程的input和output（图中灰色线），会导致大量tensor不能释放，占用存储<ul>
<li>更复杂的启发式规则，决定图计算顺序</li>
<li>recomputing tensor，而不是存储</li>
<li>将较长生存周期的tensor交换到cpu memory, 释放Gpu memory.<br><img src="/2017/12/31/tf-tutorial/gradient_graph.png" alt="gradient_graph"></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Partial-Execution"><a href="#Partial-Execution" class="headerlink" title="Partial Execution"></a>Partial Execution</h3><p>TF支持执行subgraph, 并且支持在任意地方指定input tensor. 比如debug的时候，人为指定某个tensor的值。如图所示：<br><img src="/2017/12/31/tf-tutorial/partial_excu.png" alt="partial_excu"></p>
<ul>
<li>feed: 指定替换的Input tensor 的 name[:port]; 如图中inputs={b}</li>
<li>fetch: 指定观测结果的output tensor的 name[:port];如图中outputs={f:0}</li>
<li>执行重写后的graph，其余无关节点不执行计算</li>
</ul>
<h3 id="Parallel-Techs"><a href="#Parallel-Techs" class="headerlink" title="Parallel Techs"></a>Parallel Techs</h3><h4 id="Queue"><a href="#Queue" class="headerlink" title="Queue"></a>Queue</h4><ul>
<li>允许Graph的不同部分异步执行。（Ready Queue）</li>
<li>Enqueue, Dequeue</li>
<li>example: prefetch data for some nodes from disk while other nodes are computing according the Graph.</li>
<li>grouping</li>
<li>FIFO queue, shuffle queue</li>
</ul>
<h4 id="Data-Parallel-Training"><a href="#Data-Parallel-Training" class="headerlink" title="Data Parallel Training"></a>Data Parallel Training</h4><p>复制model(part of the Graph)到不同的devices，然后并行执行。</p>
<ul>
<li>SGD: mini-batch的并行执行<br><img src="/2017/12/31/tf-tutorial/data_parallel.png" alt="data_parallel"></li>
</ul>
<h4 id="Model-Parallel-Training"><a href="#Model-Parallel-Training" class="headerlink" title="Model Parallel Training"></a>Model Parallel Training</h4><p>模型的不同部分（different portions of the Graph）在不同的devcies上并行，处理同一批数据</p>
<ul>
<li>deep LSTM<br><img src="/2017/12/31/tf-tutorial/model_parallel.png" alt="model_parallel"></li>
</ul>
<h4 id="Concurrent-Steps-for-Model-Computation-Pipelining"><a href="#Concurrent-Steps-for-Model-Computation-Pipelining" class="headerlink" title="Concurrent Steps for Model Computation Pipelining"></a>Concurrent Steps for Model Computation Pipelining</h4><p>在同一个device内部进行流水线执行。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://luoxc.com/2017/12/31/tf-tutorial/" data-id="cjc3i7upp000u3y5d1io4fxd8" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-information-history" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="2017/12/30/information-history/" class="article-date">
  <time datetime="2017-12-30T15:52:27.000Z" itemprop="datePublished">2017-12-30</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="categories/逻辑的引擎/">逻辑的引擎</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="2017/12/30/information-history/">《信息简史》--信息的本质</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <hr>
<p>　　该文主要来自于美国科普作家詹姆斯·格雷克的《信息简史》的摘要（他还写过一本畅销书《混沌：开创新科学》），观点非常深刻清晰，颇具启发性；前半部观点新奇，后半部观点深刻，深刻到我无法完全看懂，以至于看了多遍，仍没有自信以自己的语言来重构他的全部观点。不过，它可是完全勾起了我对<strong>信息论</strong>的兴趣，对香农的远见（还有书中描写的维纳的狭促）印象无比深刻，以至于屁颠地跑去找了香农信息论的原始论文阅读：《The Mathematical Theory of Communication》（通信的数学原理），看得似懂非懂意犹未尽（也可能自己的数学储备知识还不够，主要是关于马尔科夫链和随机过程的一些障碍）。另外，还发现了一本极好的书<a href="https://book.douban.com/subject/1789534/" target="_blank" rel="external">《Information Theory, Inference, and Learning Algorithms》</a>，作者是<a href="https://book.douban.com/search/David%20J.%20C.%20MacKay" target="_blank" rel="external">David J. C. MacKay</a>，只是可惜2017年被ESL缠斗太久，无暇去看。</p>
<p>　　另外，书中关于艾伦·图灵与香农的一段神交，蔡廷和科尔戈莫洛夫关于扩展熵的定义殊途同归，蔡廷关于随机性与熵与图灵的不可计算数的联系，哥德尔不完备性与海森堡不确定原理之间的关系，太多联想充斥在脑中。一种不经意的喜悦，夹杂着无知与渴求的感觉。</p>
<p>　　最后，顺便推荐这本图灵的传记<a href="https://book.douban.com/subject/10779604/" target="_blank" rel="external">图灵的秘密 : 他的生平、思想及论文解读</a>。只看了一小部分，等有空了一口气看完，再做笔记。等关于随机性、不可计算数、哥德尔不完备性、计算理论、信息论都有一定的根基了，再来改写这篇笔记，多写写自己的见解了。</p>
<h3 id="摘录"><a href="#摘录" class="headerlink" title="摘录"></a>摘录</h3><ul>
<li>而对它（信息）加以简化、精炼，并以比特度量后，人们发现信息几乎无处不在。香农的理论在信息与不确定性、信息与熵，以及信息与混沌之间架起了桥梁。</li>
</ul>
<ul>
<li><p>词典为文字的持久性提供了正式认定，它表明一个字词的意义来自于其他的字词。这意味着，所有的字词聚集到一起，就可以形成一种互相关联的结构，因为所有的字词都是由其它的字词来定义的。这种现象在口语文化中并不存在，因为在那里语言是不可见的。只有当印刷术以及词典使语言成为一个个凸起的字符，可以被细细查看时，人们才能逐渐认识到字词的意义是互相依存，甚至是循环定义的。这时，字词必须被当做字词来看待，不同于所代表的事物，代表的只是其它字词。</p>
</li>
<li><p>词汇是对共同经验的一种量度，而后者来源于互连通性。</p>
</li>
<li><p>威尔金斯正试图接近信息最纯粹、最一般的形式，而文字只不过是其中一个特例：因为我们必须意识到，一般说来，任何能够创造出有效的差异，可为某种感官所感知的东西，都足以作为表达思想的手段。</p>
</li>
<li><p>语言至此被视作服务于两种独立的功能，既是表达的工具，也是思维的工具。人们一般假定，其中思维在先。对于布尔来说，逻辑就是思维，是思维经过打磨和提纯的结果。</p>
</li>
<li><p>我的办公室里就有一部电话，不过更多的是用来展示。如果我想发条讯息的话，我可以使用发报机，或差一个童仆去跑腿就行了…… 人们在面对一种全新技术时通常会遭遇想象力失灵。人们对于电报已经很熟悉，但与之相关的经验却无法移用到电话这种新设备上来。</p>
</li>
<li><p>当可连通的电话数量超过了某个临界值时，其拓扑结构就发生了变化，而且这种变化来得异常迅速。社区性的电话网络出现了，其相互连通的功能由一种称为交换机的新型设备来管理。</p>
</li>
<li><p>如果把图灵机简化到只剩下一张有限的状态表以及一个有限的输入集，那么图灵机本身就可以用数来表示… 图灵给他的机器编码，就如同哥德尔给他的符号逻辑语言编码一样。如此这般，数据和指令之间区分就被消除了：说到底，它们都不过是数而已。每个可计算数，必定对应着一个机器编号。</p>
</li>
<li><p>任何用于生成公式的机械的流程，本质上都是一台图灵机。因此，任何形式体系中必然存在不可判定的命题。数学是不可判定的，其不完全性来源自不可计算性。</p>
</li>
<li><p>当数被拿来编码机器的行为时，悖论就会再次现身。这涉及不可避免的递归纠缠：被计算的实体与进行计算的实体纠缠到了一起，带来种种恶果…物理学同样新近遇到了类似的难题：海森堡不确定原理… 过去我们一直假定，在科学中，只要知道了宇宙在某一时刻的全部状态，我们就能把宇宙所有的未来状态都预测出来…但更为现代的科学却认为，当我们面对原子和电子时，我们无法知道它们的全部确切状态，因为我们所用的仪器本身就是有原子和电子构成的。</p>
</li>
<li><p>图灵和香农都在使用编码，只是图灵把指令编码成数，将十进制数编码成0和1，而香农是对基因、染色体、继电器和开关编码。他们的灵巧智慧都应用在了如何将一类事物映射到另一类事物（例如，代数函数与机器指令，逻辑运算与电路），也就是找出两类事物之间严格的对应关系上。<strong>在他们心智的武器库中，符号运算以及映射的思想占据着举足轻重的地位。</strong></p>
</li>
<li><p>在香农看来，一条讯息就像一个动力系统，它的未来走向会受到过去历史的影响。</p>
</li>
<li><p>计算一个系统（如热力学系统）所有可能组合，可以发现其中无序状态远多于有序状态… 因此，有序状态的熵低，出现概率也低。</p>
</li>
<li><p>熵就成了概率在物理学上的等价物：某一给定宏观状态的熵，就是它所对应的围观状态数目的对数。因此，热力学第二定律揭示的是，宇宙从可能性较小的（有序的）宏观状态演化为可能性较大的（无序的）宏观状态的趋势。</p>
</li>
<li><p>它（麦克斯韦的妖）能看见，热力学第二定律只是在统计意义上成立，而不是由某种物理原因所决定的。事实上，在分子水平，这条定律就会被随机地违背。而这个妖则是用具有目的性的行为替代了这种随机性。<strong>它用信息降低了熵</strong>。</p>
</li>
<li><p>神经系统本身的存在，就是依赖于能量的持续耗散。</p>
</li>
<li><p>信息不是免费的。麦克斯韦、汤姆森等人都默认知识是现成的–关于分子速度和轨迹的知识就直接摆在了这个妖的眼前。他们没有考虑到，获取这些信息是需要成本的… 信息是物理的。麦克斯韦妖则在两者之间架起了桥梁，它每处理一个粒子，就是做了一次信息与能量的转换。齐拉特发现，只要精确核算每次度量和记忆，这种转换也是可以精确计算的。</p>
</li>
<li><p>“我考虑的是，从一个集合中作出选择时会有多少信息产生– 这样一来，集合越大，产生的信息越多。而你考虑的是，集合越大，不确定性越高时，对于该情况的知识就越少，因而信息也就越少。”换句话说，H度量的是出人意料的程度。</p>
</li>
<li><p>我们都像麦克斯韦妖一样活动。生物体(organism)，顾名思义，时刻在组织(organize)…收发邮件，创作诗词，音乐，制造工具等。</p>
</li>
<li><p>讯息越有规律，就越可预测；越可预测，就越冗余；越冗余，含有的信息就越少。随机程度如何与含有多少信息其实是同一个问题。</p>
</li>
<li><p>为什么说$\pi$不是随机的呢？蔡廷给出了一个明确的回答：一个数只要是可计算的，即它能够被一个可定义的计算机程序生成，那它就不是随机的。因为，可计算性是随机性的一种度量。</p>
</li>
<li><p>柯尔莫哥洛夫描述了三种度量（信息的）途径：基于组合、基于概率，以及基于算法… 以避免考虑所有可能对象组成的系统时可能遇到的问题。这种途径关注对象本身。</p>
</li>
<li><p>当人或计算机从经验中学习时，他们是在使用归纳推理，从无规律的信息流里识别出规律来。从这个意义上说，科学定律其实就是一种数据压缩，而理论物理学家就像是一个非常聪明的编码算法。</p>
</li>
<li><p>所罗门洛夫感兴趣的是归纳推理：给定一个观察数据的序列，人们如何作出关于后续事件的最优预测？柯尔莫哥洛夫寻找的则是随机性的数学定义：通过掷硬币以相同概率生成的两个序列，说一个序列比另一个序列更随机是什么意思？而蔡廷试图借助图灵和香农的理论，找到另一条更深刻地认识哥德尔不完全性定理的途径，正如他后来所说，“将香农的信息论和图灵的可计算理论倒进调酒器里，然后用例晃动”。最终他们三人的答案都与最短程序的长度有关，与复杂性有关。</p>
</li>
<li><p>信息不是不具实体的抽象，而总是与物理载体相联系，因而也必须遵循物理定理…无论它是表现为石板上的一个刻记、打孔卡片上的一个孔洞，还是一个粒子的自旋，信息都不可能摆脱某种物理载体而独立存在… 兰道尔提出，只有不可逆的操作，才会导致熵的增加。</p>
<p>许多计算可以不耗费任何能量就能完成，而热量耗散也只有在擦除信息时才会发生。擦除是一种不可逆的逻辑操作… 遗忘需要功。</p>
</li>
</ul>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><ul>
<li>作者简介：<a href="https://en.wikipedia.org/wiki/James_Gleick" target="_blank" rel="external">https://en.wikipedia.org/wiki/James_Gleick</a></li>
<li>chaos software: <a href="http://www.cs.sjsu.edu/~rucker/chaos.htm" target="_blank" rel="external">http://www.cs.sjsu.edu/~rucker/chaos.htm</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://luoxc.com/2017/12/30/information-history/" data-id="cjc3i7uow000c3y5dlwrucfjy" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-influence" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="2017/12/30/influence/" class="article-date">
  <time datetime="2017-12-30T15:46:43.000Z" itemprop="datePublished">2017-12-30</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="categories/人文的意义/">人文的意义</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="2017/12/30/influence/">《影响力》摘要</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>该文来自于《影响力》一书，书有点冗长，适合速度；暂且摘录一些有用的tips。</p>
<h3 id="主要内容"><a href="#主要内容" class="headerlink" title="主要内容"></a>主要内容</h3><ul>
<li>互惠</li>
<li>承诺和一致</li>
<li>社会认同</li>
<li>喜好</li>
<li>权威</li>
<li>短缺</li>
</ul>
<h3 id="只言片语"><a href="#只言片语" class="headerlink" title="只言片语"></a>只言片语</h3><ul>
<li>我们容忍这种美中不足（指根据某些“启动特征”做出的自动反应），多半也因为我们没有更好的办法。如果不借助这些经验和公式，我们只会永远裹足不前– 站在原地分类、评价、比较–而可以让我们采取行动的时间却在毫不留情的逝去。<strong>当我们的生活空间变得越来越充满了刺激和复杂多变时，我们也必然越来越频繁地依赖捷径来应付这一切</strong>。</li>
<li>当人们有这种愿望、也有这个能力去对有关的信息进行分析时，他们更可能对这些信息做出有控制的反应；否则的话，更可能采取“咔嗒，哔”的反应方式…当人们觉得事不关己时，他们完全依赖“专家的话一定是对的”这一规则，说服他们的不是演讲本身，而是演讲者在教育领域的造诣和地位；而当人们会受到这一主张影响时，他们的态度就不一样了。</li>
<li><strong>对比原理</strong>影响我们对先后接触到的两件东西的差别的判断。比如中介让你先看很差但是价钱又稍高的房子。<h4 id="互惠原理"><a href="#互惠原理" class="headerlink" title="互惠原理"></a>互惠原理</h4></li>
<li>由于<strong>互惠原理</strong>的影响力，我们感到自己有义务在将来回报我们收到的恩惠、礼物、邀请等等。对这一类东西的接受往往与偿还的义务紧紧联系在一起… 我们人类社会发展成今天的样子，是因为我们的祖先学会了在一个以名誉作担保的义务偿还网络中分享他们的食物和技能。</li>
<li>著名法国人类学家马塞尔·毛斯在描述人类文明的送礼过程以及与此有关的社会压力时说过这样的话：<strong>给予是一种义务，接受是一种义务，偿还也是一种义务</strong>。</li>
<li>互惠原理的例子：埃塞俄比亚给墨西哥的捐款、商家的试用试吃服务、调查问卷附带的小礼品、甚至街边硬塞给你的礼物等。</li>
<li>一般来说，整个社会对不遵守互惠原理的人的确有一种发自内心的厌恶。为了避免被贴上像忘恩负义这一类的标签，即使是不公平的交换我们有时候也是愿意接受的。</li>
<li>互惠原理导致的<strong>拒绝–退让</strong>：如果他人对我们做出了让步，我们也有义务做出让步。因为妥协也可以是一个互惠的过程。</li>
<li>如果他人最初的提议是我们想要的，我们就接受它，但我们接受的只是这个提议本身，与他人的角色无关..<strong>.互惠原理指出恩惠必须用恩惠来报答，但并没有说诡计也必须用恩惠来回报</strong>。</li>
<li>那些反向破坏互惠原理的人–<strong>只给与却不给人回报的机会– 也会遭致人们的厌恶</strong>。<h4 id="承诺和保持一致"><a href="#承诺和保持一致" class="headerlink" title="承诺和保持一致"></a>承诺和保持一致</h4></li>
<li>在很多情况下保持一致都是一种很有益、很得体的行为；自相矛盾被认为是一种不良的品性。若果一个人的信仰、言辞、行为相互矛盾，这个人就会被看做优柔寡断、头脑混乱、两面三刀。而另一方面，高度的一致则是和坚强的个性和优越的智力联系在一起的，是逻辑、理性、稳定和诚实的核心。</li>
<li>一旦我们对一件事情做出了决定，固执地坚持这个决定就成了一种非常难得的奢侈，因为这意味着我们再也不用为这件事操心了。</li>
<li>这种从小的请求开始最终达到对大请求的依从的策略，叫做“<strong>入门策略</strong>”：一旦他同意了某个请求，他的态度就变了，会尽可能的去保持承诺和一致。</li>
<li><strong>一个承诺必须是积极的、公开的、经过努力才做出的、而且是人们自由选择的结果</strong>…做出一个承诺所需付出的努力越多，这个承诺对许诺者的影响就越大,要让这些人从内心深处对这个承诺负起责任来。<h4 id="社会认同原理"><a href="#社会认同原理" class="headerlink" title="社会认同原理"></a>社会认同原理</h4></li>
<li><strong>不确定性</strong>：当人们对自己的处境不是很有把握时，更有可能根据他人的行为来决定自己应该怎么办。<strong>相似性</strong>：与我们类似的人对我们最有影响力。</li>
<li>现象：当单独一个人看到门底冒烟时，75%的人报了警。然而同样的冒烟事件被一个三人小组看见时，报警的概率则降到了38%。但是，报警次数最少的时候还是当三人小组中有两人事先被告知要表现得若无其事的时候，这时候报警的概率降到了10%</li>
<li><strong>多元无知效应</strong>在陌生人之间表现得最为显著：当旁观者不能肯定他们的目击事件是否紧急，尤其是这群人互不相识时，不施以援手的可能性最大。</li>
<li>（<strong>当出现紧急事件时，）从人群中仅仅挑出一个人来，注视着他，指着他，直接对他说：“你，穿蓝夹克的先生，我需要帮助，请叫一辆救护车来。</strong>”</li>
<li><strong>维特效应</strong>：大量报到自杀事件，会导致自杀事件发生的概率。</li>
</ul>
<h4 id="权威"><a href="#权威" class="headerlink" title="权威"></a>权威</h4><ul>
<li>具有独立思考能力的成年人也会为了服从权威的命令而做出一些完全丧失理智的事情。<strong>在权威的强大压力面前，个人的抵抗是十分渺小的</strong>。</li>
<li>来自权威的信息在很多情况下都为我们提供了一条行动的捷径。如老师和父母。一方面是他们拥有更多的智慧，另一方面也是由于他们就是决定赏罚的人。</li>
<li>权威影响例子：电刑实验、R-ear眼药水</li>
<li>（抵抗权威影响）最基本的方法是对权威保持高度的警觉。有了这种警觉，同时也意识到权威是多么容易造假，我们自然会采取一种比较谨慎的态度。</li>
<li>问自己两个问题。<strong>第一，这个权威是不是真正的专家？第二，这个权威会不会对我们说真话</strong>？</li>
</ul>
<h4 id="短缺"><a href="#短缺" class="headerlink" title="短缺"></a>短缺</h4><ul>
<li>去爱一样东西的方法之一就是意识到它可能会失去。</li>
<li>稀缺的例子：有瑕疵的邮票、错版的硬币、被封禁的信息等。</li>
<li>具有讽刺意义的是，对这种人–如一些可疑的政治团体–来说，为了让人们接受他们的观点，最有效的策略不是去公开宣扬这些观点，而是故意让这些观点遭受官方的封杀，然后再把被封杀的消息公之于众。</li>
<li><strong>先拥有后失去的短缺威力更大</strong>。当经济条件和社会条件改善之后又发生短暂而急剧的倒退时，就是我们最有可能看到革命和动乱的时候。因此，在一个社会中，最容易揭竿而起的，并不是那些传统上受压迫最深的人，因为对他们来说，自己所受的压迫可能已经成了自然秩序的一部分。相反，革命者更可能是那些至少品尝过比较好的生活滋味的人。当他们亲身经历过并寄予厚望的经济上的和社会上的进步突然变得可望而不可及时，他们便会对这种进步产生比以前任何时候都更强烈的欲望，甚至不惜以暴力来保卫。</li>
</ul>
<h4 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h4><ul>
<li>他们（依从业者）真正的罪状，也是我们不能容忍的一点，是他们赚钱的方法威胁到了我们的捷径的可靠性。<strong>我们必须借助稳妥可靠的捷径和经验来应付令人眼花缭乱的现代生活，这不是一种奢侈，而是一种必要</strong>。而且随着生活步调的加快，这些经验和捷径会显得越来越重要。因此，当我们看到有人为了自己的私利而践踏这些规则时，应该义愤填膺地对他们进行反击和声讨。</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://luoxc.com/2017/12/30/influence/" data-id="cjc3i7uoj000b3y5d3ozy956q" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-funny-equation-1-variance-decomposition" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="2017/09/17/funny-equation-1-variance-decomposition/" class="article-date">
  <time datetime="2017-09-17T05:51:46.000Z" itemprop="datePublished">2017-09-17</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="categories/机器的理智/">机器的理智</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="2017/09/17/funny-equation-1-variance-decomposition/">好玩的公式系列1：方差分解的联想</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>在“A FIRST COURSE IN PROBABILITY”里，偶然看到如下的条件方差分解公式：<br>$$Var(X) = Var(E[X|Z])+ E_X[Var(X|Z)]$$<br>于是我换了变量名，一下子熟悉起来：<br><strong>$$Var(Y) = Var(E_Y[Y|X])+ E_X[Var(Y|X)] $$</strong></p>
<ul>
<li>在机器学习中，X看做特征，Y为目标；在统计里，X看做自变量，Y看做因变量</li>
<li>它表达了理想情况下，采用<strong>MSE准则</strong>，预测函数$Y=g(X)=E_Y[Y|X]$时的方差组成：<ul>
<li>可以解读为: $target_var =  captured_var + MSE$<ul>
<li>$ Var(E_Y[Y|X])$: $captured_var$, 表征预测模型捕获的方差。</li>
<li>$ E_X[Var(Y|X)] $：表征预测残差，此时的MSE只有噪声，没有模型偏差。</li>
<li>$R^2 = \frac{Var(E_Y[Y|X])}{Var(Y)} $: 统计学里称之为<strong>“决定系数”</strong>。$R^2$越接近1，则表明$Y$的方差几乎都被$g(X)$捕获，预测残差小，预测的效果好；也说明数据质量好，噪音小！在这种理想情况下($g(X) = E_Y[Y|X]$)，可以用$R^2$评估数据质量。</li>
</ul>
</li>
</ul>
</li>
<li><p>事实上，往往预测函数$Y=g(X)\  \neq E_Y[Y|X]$：</p>
<ul>
<li>预测函数不能有效捕获Y的方差，MSE增大：$captured_var &lt; Var(E_Y[Y|X])$，$MSE &gt; E_X[Var(Y|X)]$,<ul>
<li>MSE的误差来源包含两部分：固有噪声$E_X[Var(Y|X)]$（机器学习中称为<strong>方差variance</strong>）和模型误差（机器学习中称为<strong>偏差bias</strong>），具体见下文证明。</li>
<li>若$g(X)$太差，将导致模型偏差部分非常大！机器学习中称之为“<strong>欠拟合</strong>”，下面是可能的原因：<ul>
<li>数据量过小，不能有效的估计出$E[Y|X]$</li>
<li>特征太差，没有表征力（极端情况下X和Y独立，见下文分析）</li>
<li>假设空间容量不够大，没有$E_X[Y|X]$及其有效近似。（如采用线性拟合非线性数据）<ul>
<li>假设空间足够大，但是没有有效的搜索策略。（如nerual network高度非凸的搜索空间）</li>
</ul>
</li>
</ul>
</li>
<li>要防止在训练集上过度拟合$g(X)$：<ul>
<li>机器学习中称之为<strong>“过拟合”</strong>：在训练集上近乎完美的拟合数据，但在测试集上表现糟糕, 因为：</li>
<li>现实数据集往往是有限的、有噪声的<ul>
<li>数据有限：极有限的数据可能难以反应真实数据分布， $g(X)$可能会拟合出过于复杂但不真实的形状，它可能会造成方差和偏差都增大；</li>
<li>数据噪声：噪声往往来源于固有噪声、数据收集噪声(如生产系统上的各种错误导致label不正确)等。这里只考虑固有噪声，即Y并不是X的确定性函数（如MSE假设$y = f(x) + \delta, \ \delta \sim N(0, \sigma)$）。通常，完美拟合训练集，$g(X)$会拟合噪声的波动，导致函数过于复杂，极小的$\Delta x$，会带来极大的$\Delta g(x)$；即我们通常所说的“方差过大，导致过拟合”</li>
<li>一般而言，我们在减小偏差(解决欠拟合)时，要防范方差爆炸（防范过拟合），可以采取正则化,drop out，bagging等一系列措施。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>当$X$与$Y$独立时（即特征X太烂，无法表征目标Y）: </p>
<ul>
<li>公式右边退化为$Var(Y) + 0$<ul>
<li>预测函数将完全无法捕获$Y$的方差</li>
<li>MSE中的偏差部分将增大！（特征工程很重要）</li>
</ul>
</li>
</ul>
</li>
<li><p>如何更好的使$g(X)$逼近$E[Y|X]$呢？</p>
<ul>
<li>对于条件期望$E[Y|X]$, 若对于每一个可能的$X$, 都有足够多样本来估计Y的条件期望；之后的预测，就只需查表。显然，不符合现实。<ul>
<li>对每个点$X$我们无法得到足够样本，但是可以将X的空间分段，进行分段拟合；</li>
<li>分段拟合为防止过拟合，可以限定假设空间为3次自然样条。</li>
<li>KNN采用$g(x) = Avg(y_i\ |\ x_i \in N_k(x))$来近似$E[Y|X=x]$</li>
</ul>
</li>
</ul>
</li>
<li><p>在抽样理论中：可尝试使用随机变量$E_Y[Y|X]$ 代替$Y$来模拟$E[Y]$,减小抽样方差， 因为：</p>
<ul>
<li>$E_X[E_Y[Y|X]] = E[Y]$<ul>
<li>$Var(E_Y[Y|X]) \le Var(Y)$</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>MSE预测：</strong>对于MSE损失函数$L = E[Y-g(X)]^2$的解为：$g(X) = E[Y|X]$<br><strong>引理：</strong> $$\forall g(X), E[(Y-g(X))^2 | X] = \<br> E[(Y- E[Y|X])^2 |X] + E[(E(Y|X) - g(X))^2|X] \<br> \ge  E[(Y- E[Y|X])^2 |X]$$</p>
<ul>
<li>$ E[(Y- E[Y|X])^2 |X] $：数据分布固有噪声,如高斯白噪声等。称之为方差variance。</li>
<li>$E[(E(Y|X) - g(X))^2|X] $:  模型误差部分，称之为偏差bias。如真实函数为二次，却用线性回归拟合。模型误差受限于假设空间的容量，也受限于在假设空间的搜索过程（可能是局部最小值，并不能得到最优的$E(Y|X)$）</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://luoxc.com/2017/09/17/funny-equation-1-variance-decomposition/" data-id="cjc3i7unl00043y5di3samswp" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-godel-proof" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="2017/09/17/godel-proof/" class="article-date">
  <time datetime="2017-09-17T02:28:30.000Z" itemprop="datePublished">2017-09-17</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="categories/逻辑的引擎/">逻辑的引擎</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="2017/09/17/godel-proof/">哥德尔不完全性</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>　　在几何学的历史上，有一段传奇公案：欧几里得第五公设（平行公理）的研究。<br>　　“过直线外一点，有且仅有一条直线与已知直线平行” – <strong>欧氏几何</strong><br>　　不同于其它四条公理，平行公理并非那么显然为真。大量数学家涌入该领域，希望：要么证明平行公理是从其它公理推导出来的定理，要么证明平行公理是不可或缺的。<br>　　可是，这两条途径都失败了。少数天才数学家意识到，平行公理既无法证明是定理，也并非不可或缺。非欧几何就此诞生。罗巴切夫斯基提出“罗氏几何”，黎曼提出“黎曼几何”（后用于解释广义相对论）。<br>　　“过直线外一点，至少可以做两条直线与已知直线平行” – <strong>罗氏几何</strong><br>　　“过直线外一点，没有直线与已知直线平行” – <strong>黎曼几何</strong><br>　　其时，意识到非欧几何的，还有更早的匈牙利21岁的鲍耶·雅诺什，以及人称数学王子和老狐狸的高斯。高斯可能是最早意识到非欧几何的存在，只是太违反现实直觉，而迟迟没有公布相关研究成果。“点”，“线”，“平面”的定义，已经在我们脑海里根深蒂固。但我们如果抛开成见，在球面上理解“黎曼几何”，将“点”理解为球面上一条直径的两个端点，一切都将是相容的。“点”如何定义无关紧要，重要的是据此而来的推理的相容性。<br>　　对于公理系统来说，公理是否自明为真无关紧要，重要的是推导出来的定理的相容性。对于形式化公理系统来说，有两个核心问题，“一致性”，“完备性”。<br>　　“一致性是指，从公理出发，不会推导出命题P和~P同时为真”<br>　　“完备性是指，该公理系统中真命题，都能够从公理系统推导出”<br>　　形式系统已经将现实意义残忍剥离，而“一致性”和“完备性”就成了数学家最朴素的愿望，尤其是“一致性”。罗素和怀特海则在鸿篇巨著《数学原理》中将数学系统抽象成纯逻辑系统；大卫·希尔伯特提出了“一致性”的绝对证明原则。<br>　　可是，希尔伯特和罗素的愿望，被当时的青年才俊无需担心买房的哥德尔无情击破。1931年，维也纳大学时年25岁的库尔特·哥德尔，发表“论《数学原理》及相关系统的形式不可判定命题”，成为逻辑和数学史上的一座里程碑。哥德尔采用了类似于“说谎者”悖论的论证逻辑。（或者“理发师悖论”。具体什么悖论，结果都会殊途同归，只要这些悖论谈论他们自身。有时间会整理各种有意思的悖论和思想实验）<br>　　“我说的这句话是假的。” 这个命题是不可判定的。<br>　　哥德尔给了数学家、逻辑学家、哲学家几乎是致命的一击，以至于伟大的语言哲学家维特根斯坦终生拒绝相信哥德尔的结论（就是那个博士答辩，让导师罗素、摩尔、魏斯曼不敢开口提问的天才。他的传记参见《维特根斯坦传：天才之为责任》）。<br>　　哥德尔不完备定理的证明核心有两点：</p>
<ul>
<li>哥德尔映射（同构）：借助哥德尔配数法，将形式逻辑系统内的合法公式映射为整数（哥德尔数）；将形式系统的元命题映射为整数之间的算术性质！借助于算术的桥梁，谈论该系统的某个元命题，其实就是该系统内的某个公式。形式系统有了忠实谈论自身的能力（自指的魔力，带来悖论的魔力）。</li>
<li>构建元命题的公式G：“G在本形式系统不可证”</li>
</ul>
<p>最终，哥德尔得到了如下的不完备定理：</p>
<ul>
<li><strong>哥德尔第一不完备性定理</strong>：任何包含算术系统的形式系统S（包括下文的PM）若是一致的，则是不完全的。<br>  它说明，对于形式公理系统，“一致性”和“完备性”不可兼得。“一致性”是一个公理系统能用于解释现实的最基本要求，在不一致的系统里，任何合法命题都是定理，这样的公理系统没有意义。系统的“不完备性”说明，总有一些“真理”，根据我们的公理系统，是不可判定的－－也许，“哥德巴赫猜想”就是我们的数论系统无法判定的“真命题”。</li>
<li><strong>哥德尔第二不完备性定理</strong>： 如果形式系统S包含算术系统，当S一致时，S的一致性无法在S内部证明。<br>  更令人沮丧的是，即使我们放弃了“完备性”，“一致性”也无法仅在该公理系统内得到证明。通俗的说，我们无法使用算术系统的武器来证明算术系统的一致性。我们只能求助于算术系统之外的“元数学证明”。可问题是，我们将S纳入更强的形式系统S’， S’还是会逃不脱这两个不完备性定理。鱼和熊掌，不可兼得；放弃了熊掌，然而我们并不能证明得到的鱼就一定是鱼… </li>
</ul>
<p>整个证明的轮廓参照《哥德尔证明》简述如下：</p>
<h3 id="一致性问题"><a href="#一致性问题" class="headerlink" title="一致性问题"></a>一致性问题</h3><p><strong>公理化运动：</strong>（如欧几里得第五公设） 从事纯数学研究的适当方法，就是从假设的公理前提推到出定理，而这些公理是否自明为真则无关紧要。</p>
<ul>
<li><strong>抽象</strong>：原则上数学命题可以表达任何东西；</li>
<li><strong>形式化：</strong>数学证明的有效性只基于命题结构，与具体事物特性无关。</li>
<li><p><strong>一致性问题：</strong> 一组公理是否是内部相容的，是否能确保不会推导出矛盾的定理。</p>
<ul>
<li>基本思想：为抽象系统找合理的“模型”或“解释”，使每一条公设为真，若该”模型“是一直的，则该抽象系统是一致的。</li>
<li>存在的问题：将一致性的证明转嫁到”模型”上<ul>
<li>若有多个相容的解释呢？即公设不完备的时候。如文中“三角形公理系统”中去掉公设2，则所有不少于3个点完全图都是一个解释；那么有些命题将是无法判定的？比如元素个数相关的命题。</li>
<li>平面椭圆几何的一致性：平面-&gt;欧几里得球面；线-&gt; 大圆；点-&gt; 一对对顶点。</li>
<li>几何公设代数化：利用笛卡尔坐标，将集合关系转化成代数关系。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>完备性问题：</strong> 对一个形式化系统中所有为真的公式，都能从该系统的公理中推理出来。</p>
</li>
</ul>
<h3 id="一致性的绝对证明"><a href="#一致性的绝对证明" class="headerlink" title="一致性的绝对证明"></a>一致性的绝对证明</h3><p><strong>元数学：</strong> 针对演绎统（形式系统）的命题</p>
<ul>
<li><strong>元数学命题：形式系统x是一致的</strong>。<ul>
<li>演绎系统形式化；只按规则演绎。形式演算与“解释”分离。</li>
<li>在证明一致性过程中，不能涉及到公式的无限数量的结构性质，也不能涉及到对公式的无限个操作运算。</li>
</ul>
</li>
</ul>
<h3 id="一致性绝对证明的例子"><a href="#一致性绝对证明的例子" class="headerlink" title="一致性绝对证明的例子"></a>一致性绝对证明的例子</h3><p>形式化的四步骤(以一个简单形式系统举例)：</p>
<ol>
<li>符号集（词汇）<ul>
<li>命题变量： p, q, r等变量等，可用命题带入</li>
<li>常量符号：<ul>
<li>命题联结符：~ $\lor$  $\supset$ $\cdot$</li>
<li>标点符号：(  )</li>
</ul>
</li>
</ul>
</li>
<li>公式：合法的形式字符串（语法）<ul>
<li>命题变量是公式</li>
<li>若S是公式，使用命题联结符的组合也是公式。</li>
</ul>
</li>
<li>演绎规则<ul>
<li>替换规则：将命题变量替换为其它公式得到一个新的公式。</li>
<li>分离规则：若$S_1, S_1 \supset S_2, 则 S_2$</li>
</ul>
</li>
<li>公理<ul>
<li>$(p \lor p ) \supset p$</li>
<li>$p \supset (p \lor q)$</li>
<li>$(p \lor q) \supset (q \lor p)$</li>
<li>$(p \supset q) \supset ((r \lor p) \supset (r \lor q))$</li>
</ul>
</li>
</ol>
<p><strong>定理：</strong>能从公理根据演绎规则得到的公式，称之为定理。<br><strong>一致性</strong>：公式$S$和公式$～S$无法同时成立，否则根据定理$p \supset (～p \supset q)$可以得到$q$,即任何公式都是定理；也就是说，<strong>只要证明至少一个公式不是定理，那么该系统就是一致的</strong>。</p>
<p><strong>一致性证明过程：</strong> “重言”是四个公理都具备的性质，且具有遗传性，即所有定理都会有“重言”这一性质，但公式”$p \lor q$”不是重言的，因此不是定理，即说明上述形式系统时一致的。</p>
<h3 id="哥德尔证明"><a href="#哥德尔证明" class="headerlink" title="哥德尔证明"></a>哥德尔证明</h3><h4 id="1-哥德尔配数法"><a href="#1-哥德尔配数法" class="headerlink" title="1. 哥德尔配数法"></a>1. 哥德尔配数法</h4><ul>
<li>12个常量符号：1 ~ 12</li>
<li>三种变量：<ul>
<li>数字变量：大于12的不同素数</li>
<li>命题变量：大于12的不同素数的平方</li>
<li>谓词变量：大于12的不同素数的立方</li>
</ul>
</li>
<li>公式：将公式字符串中的字符对应的哥德尔数，当做从2依次开始的素数的指数，这些因子的乘积作为该公式的哥德尔数</li>
<li>公式序列：同上述公式处理方法，指数改成公式的哥德尔数。</li>
</ul>
<p>对于任意的哥德尔数，都能按照上述法则还原成相应的常量、变量、公式或公式序列。<br><strong>在PM系统中，严格来讲，公式没有“真”或”假”的概念，只有“是否是定理”的提法</strong>（即能否从公理推导而来），“真”或“假”是对应某些形如“ss0 = ss0”的公式的<strong>算术解释</strong>；在不严格的情况下，说公式为“真”，是指它是定理。而对于大量非公式的字符串 如”ss0+ss0”，它不是公式，更不可是定理，自然也没有真假之分的。</p>
<p><strong>哥德尔映射引理：</strong> </p>
<ul>
<li>对于PM定理组成的无穷类，其中每一条定理，如果按照通常的算术含义加以解释，都表示一个算术真理</li>
<li>对于算术真理的无穷类（<strong>原始递归算术真理</strong>），其中每一个真理转换成形式公式，就得到PM中的一条定理</li>
<li>PM能忠实的谈论自身：有关PM的元数学真命题映照为PM的定理。</li>
</ul>
<p><strong>原始递归（primitive recursive）：</strong></p>
<ul>
<li>使用复合和递归作为中心运算来定义。</li>
<li>总是停机的图灵机计算。</li>
<li>公理：常函数、后继函数、投影函数是原始递归的。<br>参见维基百科“原始递归函数”</li>
</ul>
<h4 id="2-元数学的算术化"><a href="#2-元数学的算术化" class="headerlink" title="2. 元数学的算术化"></a>2. 元数学的算术化</h4><p>一组符号串在排列上的结构性质，可以通过探寻大整数的素因子性质来表述。即<strong>元数学算术化</strong></p>
<ul>
<li>$dem(x, z)$：它对应的元命题是，哥德尔数x的公式序列是哥德尔数z的公式在PM中的证明，它是<strong>整数x和z的原始递归关系</strong>。</li>
<li>$Dem(x,z)$:表示$dem(x,z)$在PM中的<strong>形式符号公式</strong>。根据哥德尔引理，若$dem(x,z)$为真，则$Dem(x,z)$是PM中的定理。</li>
<li>$sub(x, 17, x)$: 它对应的元命题是，将一个哥德尔数为x的公式中，凡是变量为$y(17)$的地方用$x$对应的形式符号代替而得到的新公式的哥德尔数。$sub(\cdot)$是<strong>整数x和17的一个原始递归函数</strong>。</li>
<li>$Sub(x, 17, x)$: 表示$sub(x, 17, x)$在PM中对应的<strong>形式符号串</strong>。</li>
</ul>
<h4 id="3-哥德尔论证的核心"><a href="#3-哥德尔论证的核心" class="headerlink" title="3. 哥德尔论证的核心"></a>3. 哥德尔论证的核心</h4><p><strong>论证步骤：</strong></p>
<ul>
<li><p>构造PM中的公式G，对应元命题：使用PM的规则，公式G不可证。</p>
<ul>
<li>公式$\sim (\exists x)\ Dem(x, Sub(y, 17, y)) $的哥德尔数是n, 元命题：哥德尔数为$sub(y, 17, y)$的公式不可证明</li>
<li>公式$G$ 定义为$\sim (\exists x)\ Dem(x, Sub(n, 17, n)) $的哥德尔g，则$g = sub(n, 17, n)$.</li>
</ul>
</li>
<li><p>G是可证明的，当且仅当~G是可证明的。若PM是一致的，则G和~G都形式不可证，即G是一个不可判定的公式</p>
</li>
<li><p>G形式不可证明，却是真的，因此<strong>PM本质上是不完备的</strong>。</p>
<ul>
<li>当PM不一致时，从元数学论证的角度，G陈述的是真理：G在PM中不存在证明</li>
</ul>
</li>
<li><p>元命题“若PM是一致的（公式A），则公式G是不可证的”</p>
<ul>
<li>公式A: $(\exists y) \sim \ (\exists x)\  Dem(x, y) $ 元命题：PM是不一致的。</li>
<li>公式G: $\sim (\exists x)\ Dem(x, Sub(n, 17, n)) $ 元命题：G是不可证的</li>
<li>公式“$A \supset G$”在PM中是可证的。它的元命题正如题干所述。</li>
</ul>
</li>
<li>当PM一致时，公式A在PM中是形式不可证的，否则G可证。因此，<strong>PM的一致性是无法使用能映照到PM内部的逻辑推理实现的</strong>。</li>
</ul>
<h3 id="八、结论性思考"><a href="#八、结论性思考" class="headerlink" title="八、结论性思考"></a>八、结论性思考</h3><ul>
<li>创造性思想将永远体现在数学中，创造性将永远是必需的。</li>
<li>人的思想从根本上不是一种逻辑引擎，而是一种模拟引擎，一种学习引擎，一种猜测引擎，一种审美驱动的引擎，自我校正的引擎。</li>
<li>我们不应为此感到沮丧，而应把握住这个对创造性理性在此赞赏的机会。</li>
<li>每一次数学危机，都是对人类认知无比艰巨的挑战，但每一次都引导了人类智力的大爆发。从负数的引进，从无理数的发现，从欧几里得第五公设的危机，从根式可解性，从古典三大尺规作图难题，从无穷小困窘，从逻辑基础的“理发师悖论”… 每一次危机和挑战，吸引了无数青年才俊、顶尖人类智力的扎堆投入，也正因为如此，也才有了度过每一次危机之后的人类认知的巨大爆发… 就像寒武纪生物种群大爆发一样…诞生了那么多耀眼的青年才俊…于是我们有了负数、无理数、虚数、群论、微积分、集合论…</li>
</ul>
<h3 id="相关信息"><a href="#相关信息" class="headerlink" title="相关信息"></a>相关信息</h3><ul>
<li>《哥德尔证明》</li>
<li>《GEB》</li>
<li>《信息简史》</li>
<li>《通信的数学原理》</li>
<li>《混沌，开创新科学》</li>
<li>《失控》</li>
<li>《逻辑的引擎》</li>
<li>《人工智能哲学》</li>
<li>《艾伦·图灵传》</li>
<li>莱布尼茨 – 布尔– 巴贝奇 – 邱奇– 图灵– 香农– 哥德尔– 蔡廷– 所罗门洛夫–冯·诺依曼– 科尔戈莫洛夫</li>
<li>模式–随机性–信息– 复杂性–逻辑深度–可计算数–混沌</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://luoxc.com/2017/09/17/godel-proof/" data-id="cjc3i7uob00093y5d1jhjxi13" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    <span class="page-number current">1</span><a class="page-number" href="page/2/">2</a><a class="extend next" rel="next" href="page/2/">__('next') &raquo;</a>
  </nav>
</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="categories/人文的意义/">人文的意义</a></li><li class="category-list-item"><a class="category-list-link" href="categories/机器的理智/">机器的理智</a></li><li class="category-list-item"><a class="category-list-link" href="categories/琐碎的胜利/">琐碎的胜利</a></li><li class="category-list-item"><a class="category-list-link" href="categories/逻辑的引擎/">逻辑的引擎</a></li></ul>
    </div>
  </div>


  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="archives/2018/01/">January 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="archives/2017/12/">December 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="archives/2017/09/">September 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="archives/2017/04/">April 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="archives/2017/02/">February 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="2018/01/06/gan-basic/">GAN的基本原理</a>
          </li>
        
          <li>
            <a href="2017/12/31/auto-encoder/">auto_encoder</a>
          </li>
        
          <li>
            <a href="2017/12/31/optimization/">最优化方法</a>
          </li>
        
          <li>
            <a href="2017/12/31/unbias-estimator/">无偏估计</a>
          </li>
        
          <li>
            <a href="2017/12/31/norm-dist/">正态分布</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 Finch Luo<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="index.html" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="fancybox/jquery.fancybox.css">
  <script src="fancybox/jquery.fancybox.pack.js"></script>


<script src="js/script.js"></script>

  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>