<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>贝叶斯学习 | 西瓜的木屋</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="注：笔记来自于CMU Tom M. Mitchell的《machine learning》，持续更新笔记，会不断加入自己觉得有联系的知识。

贝叶斯法则$D$是给定训练数据，$h \in H$是假设空间中的一个假设，$P(h)$是先验概率，则后验概率如下：$$P(h | D) = \frac{P(h)\cdot P(D|h)}{P(D)} $$
极大后验假设MAP$$h{MAP} = argmax">
<meta property="og:type" content="article">
<meta property="og:title" content="贝叶斯学习">
<meta property="og:url" content="http://luoxc.com/2017/04/05/ml-ch6-mitchell/index.html">
<meta property="og:site_name" content="西瓜的木屋">
<meta property="og:description" content="注：笔记来自于CMU Tom M. Mitchell的《machine learning》，持续更新笔记，会不断加入自己觉得有联系的知识。

贝叶斯法则$D$是给定训练数据，$h \in H$是假设空间中的一个假设，$P(h)$是先验概率，则后验概率如下：$$P(h | D) = \frac{P(h)\cdot P(D|h)}{P(D)} $$
极大后验假设MAP$$h{MAP} = argmax">
<meta property="og:updated_time" content="2017-12-30T16:47:59.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="贝叶斯学习">
<meta name="twitter:description" content="注：笔记来自于CMU Tom M. Mitchell的《machine learning》，持续更新笔记，会不断加入自己觉得有联系的知识。

贝叶斯法则$D$是给定训练数据，$h \in H$是假设空间中的一个假设，$P(h)$是先验概率，则后验概率如下：$$P(h | D) = \frac{P(h)\cdot P(D|h)}{P(D)} $$
极大后验假设MAP$$h{MAP} = argmax">
  
    <link rel="alternate" href="/atom.xml" title="西瓜的木屋" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="../../../../css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="../../../../index.html" id="logo">西瓜的木屋</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="../../../../index.html" id="subtitle">记录关于机器学习、智能逻辑、人文社科方面的读书心得</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="../../../../index.html">Home</a>
        
          <a class="main-nav-link" href="../../../../archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="Flux RSS"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Rechercher"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://luoxc.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-ml-ch6-mitchell" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="" class="article-date">
  <time datetime="2017-04-04T16:01:39.000Z" itemprop="datePublished">2017-04-05</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="../../../../categories/机器的理智/">机器的理智</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      贝叶斯学习
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>注：笔记来自于CMU Tom M. Mitchell的《machine learning》，持续更新笔记，会不断加入自己觉得有联系的知识。</p>
<hr>
<h3 id="贝叶斯法则"><a href="#贝叶斯法则" class="headerlink" title="贝叶斯法则"></a>贝叶斯法则</h3><p>$D$是给定训练数据，$h \in H$是假设空间中的一个假设，$P(h)$是先验概率，则后验概率如下：<br>$$P(h | D) = \frac{P(h)\cdot P(D|h)}{P(D)} $$</p>
<p><strong>极大后验假设MAP</strong><br>$$h<em>{MAP} = argmax</em>{h \in H} P(h|D)  =  argmax_{h \in H} P(h)\cdot P(D|h)$$</p>
<p><strong>极大似然假设ML</strong><br>$$h<em>{ML} = argmax</em>{h \in H} P(D|h)$$</p>
<p><strong>贝叶斯最优分类器</strong><br>$v<em>j \in V$表示分类集合的一个类别，则贝叶斯最优分类器定义如下：<br>$$argmax</em>{v<em>j \in V} \sum</em>{h_i \in H}P(v_j | h_i)P(h_i | D)$$</p>
<p><strong>朴素贝叶斯分类器</strong><br>基于假设：给定目标值时，属性值之间相互条件独立,此时朴素贝叶斯等价于MAP分类<br>$$v<em>{NB} = argmax</em>{v_j \in V} P(v_j | a_1, a_2…a<em>n) =  argmax</em>{v_j \in V}P(a_1, a_2… a_n | v_j)P(v<em>j)$$<br>由独立性：<br>$$v</em>{NB} = argmax_{v_j \in V} P(v<em>j) \prod</em>{i}P(a_i | v_j)$$</p>
<p>当$P(a_i|v_j) = \frac{n_c}{n}$非常小，如0.05,而样本数量为n = 5时，$n_c$最可能的值为0， 此时：</p>
<ul>
<li>就产生偏低的概率估计；</li>
<li>当查询包含该项时，会被0 dominate</li>
</ul>
<p>因此，常采用下面的<strong>平滑方法</strong>, m为等效样本大小，p表示该属性值的先验概率，默认是$\frac{1}{k}$（k为该属性值的取值个数）<br>$$\frac{n_c + mp}{n + m}$$</p>
<h3 id="Gibbs-算法"><a href="#Gibbs-算法" class="headerlink" title="Gibbs 算法"></a>Gibbs 算法</h3><p>贝叶斯最优分类器分类时需遍历所有假设，开销过大，采用Gibbs算法：</p>
<ul>
<li>按照H上的后验概率，从H中选择h</li>
<li>利用h来预言下一实例x的分类</li>
</ul>
<h3 id="极大似然假设-ML"><a href="#极大似然假设-ML" class="headerlink" title="极大似然假设(ML)"></a>极大似然假设(ML)</h3><p>当先验概率相等时，MAP假设等价于ML假设</p>
<p><strong>最小平方误差假设</strong><br>当满足下面条件时，最小误差平方假设等价于极大似然假设</p>
<ul>
<li>训练样本独立</li>
<li>样本的目标值 $d_i = f(x_i) + \epsilon_i$, 其中噪声$\epsilon_i \sim N(0, \sigma)$独立同分布</li>
<li>只考虑目标值的噪声，但是假定属性值没有噪声</li>
</ul>
<p><strong>预测概率分布的假设</strong></p>
<p>交叉熵最小化的假设等价于极大似然的前提：</p>
<ul>
<li>目标的布尔值为输入实例的概率函数</li>
</ul>
<p>我们需要学习$P(d=1|x)$, 假设训练样本$(x_i, d<em>i), i = 1,2…m$相互独立<br>$$P(D | h) = \prod</em>{i=1}^{m}P(x_i, d<em>i | h) = \prod</em>{i=1}^{m}P(d_i|h, x_i)P(x_i)$$<br>$$P(d_i|h, x_i) = h(x_i)^{d_i}(1-h(x_i))^{1-d<em>i}$$<br>$$h</em>{ML} =argmax<em>{h \in H} \prod</em>{i=1}^{m}h(x_i)^{d_i}(1-h(x_i))^{1-d_i}P(x_i)$$<br>取对数，并去掉最后无关的$In P(x<em>i)$:<br>$$h</em>{ML} =  argmax<em>{h \in H}  \sum</em>{i=1}^{m}d_i lnh(x_i) + (1-d_i)ln(1-h(x<em>i))$$<br>$$h</em>{ML} =  argmin<em>{h \in H}  - \sum</em>{i=1}^{m}d_i lnh(x_i) + (1-d_i)ln(1-h(x_i))$$<br>即：<strong>最小化交叉熵</strong></p>
<ul>
<li>极大似然准则(ML)，等价于最小化实例数据分布和模型生成分布之间的KL距离：$KL(P<em>{data} || P</em>{model})$</li>
<li>借助GAN的框架（主要是f-GAN的思想），我们可以提出不同于ML的准则，即最小化不同于KL距离的其它概率距离度量，比如$JS(P<em>{data} || P</em>{model})$(参见origin GAN, f-GAN的原始论文)</li>
</ul>
<h3 id="最小描述长度准则-MDL"><a href="#最小描述长度准则-MDL" class="headerlink" title="最小描述长度准则(MDL)"></a>最小描述长度准则(MDL)</h3><p><strong>利用信息论中的概率重写MAP准则</strong>：<br>$$h<em>{MAP} = argmin</em>{h \in H} - log_2P(D|h) - log<em>2P(h) = argmin</em>{h \in H}L_{C<em>H}(h) + L</em>{C_H}(D|h)$$</p>
<ul>
<li>$L_{C_H}(h)  = -log_2P(D|h)$: 在假设空间H的最优编码下h的描述长度</li>
<li>$ L_{C_H}(D|h) = -log_2P(D|h)$:在上述最优编码下，给定假设h时训练数据D的描述长度</li>
</ul>
<p><strong>MDL准则:</strong> $h<em>{MDL} = argmin</em>{h \in H}L_{C<em>1}(h) + L</em>{C_2}(D|h)$<br>当$C_1 = C_H, C_2=C<em>H时， h</em>{MDL} = h_{MAP}$</p>
<ul>
<li>当假设$h$足够复杂时$L_{C<em>1}(h)$较大，每个样例都能分类正确$L</em>{C_2}(D|h)= 0$</li>
<li>反之，结论相反。MDL准则给出了<strong>假设的复杂性和假设产生的错误数量之间的折中</strong></li>
</ul>
<p>这篇论文描述了应用MDL准则进行决策树修剪的实验：<a href="http://ac.els-cdn.com/0890540189900102/1-s2.0-0890540189900102-main.pdf?_tid=823d550e-0639-11e7-ae02-00000aab0f26&amp;acdnat=1489223152_9eb9653ea481af70d478f033f7e4dda8" target="_blank" rel="external">Inferring decision trees using the minimum description lenght principle</a></p>
<h3 id="贝叶斯信念网"><a href="#贝叶斯信念网" class="headerlink" title="贝叶斯信念网"></a>贝叶斯信念网</h3><p>贝叶斯信念网借助变量之间的条件独立性，描述了一组随机变量的联合概率分布。节点表示随机变量，有向线表示依赖关系。<br><strong>条件独立性: </strong>当给定变量集合Z, 变量集合X条件独立于变量集合Y<br>$$  P(X_1…X_l | Y_1…Y_m,Z_1…Z_n) =  P(X_1…X_l | Z_1…Z_n)$$<br><strong>联合概率:</strong> 对于n个随机变量的贝叶斯信念网，有：<br>$$  P(x_1, x_2…x<em>n)  = \prod</em>{i=1}^{n}P(x_i | parents(X_i))$$<br>$parents(X_i)$表示$X_i$的直接前驱节点。</p>
<p><strong>贝叶斯网的学习</strong><br>当给定网络结构时，可以通过最大化$P(D|h)$来学习</p>
<h3 id="EM-算法"><a href="#EM-算法" class="headerlink" title="EM 算法"></a>EM 算法</h3><p>存在<strong>隐含变量</strong>时的一种学习算法：$Y$表示可观察数据，$Z$表示未观察到的数据，$X = Y \bigcup Z$表示完整数据；$Z$可以看做随机变量，它依赖于未知参数$\theta$和观测数据$Y$</p>
<ul>
<li><strong>E-step:</strong> $Q$函数, 在给定$Y$和当前参数$\theta^{(i)}$下，完全数据的对数似然函数$lnP(X|\theta)$关于未观测数据$Z$的期望($Z$此时由条件概率$P(Z|Y, \theta^{(i)})$限定)<br>$$Q(\theta, \theta^{(i)}) = E_Z[lnP(Y, Z|\theta)|Y, \theta^{(i)}]$$</li>
<li><strong>M-step:</strong>使得上述$Q$函数取极大值的$\theta$作为$\theta^{(i+1)}$</li>
</ul>
<p><strong>高斯混合模型</strong>：经典的使用EM算法的例子</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://luoxc.com/2017/04/05/ml-ch6-mitchell/" data-id="cjc3iqp7r000gu15dee2uie9v" class="article-share-link">Partager</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="../../../09/17/godel-proof/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Récent</strong>
      <div class="article-nav-title">
        
          哥德尔不完全性
        
      </div>
    </a>
  
  
    <a href="../../04/ml-ch2-mitchell/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Ancien</strong>
      <div class="article-nav-title">概念学习</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Catégories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="../../../../categories/人文的意义/">人文的意义</a></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/机器的理智/">机器的理智</a></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/琐碎的胜利/">琐碎的胜利</a></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/逻辑的引擎/">逻辑的引擎</a></li></ul>
    </div>
  </div>


  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="../../../../archives/2018/01/">January 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="../../../../archives/2017/12/">December 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="../../../../archives/2017/09/">September 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="../../../../archives/2017/04/">April 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="../../../../archives/2017/02/">February 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Articles récents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="../../../../2018/01/06/standard-varariable/">特征标准化与条件数</a>
          </li>
        
          <li>
            <a href="../../../../2018/01/06/gan-basic/">GAN的基本原理</a>
          </li>
        
          <li>
            <a href="../../../12/31/auto-encoder/">auto_encoder</a>
          </li>
        
          <li>
            <a href="../../../12/31/optimization/">最优化方法</a>
          </li>
        
          <li>
            <a href="../../../12/31/unbias-estimator/">无偏估计</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 Finch Luo<br>
      Propulsé by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="../../../../index.html" class="mobile-nav-link">Home</a>
  
    <a href="../../../../archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="../../../../fancybox/jquery.fancybox.css">
  <script src="../../../../fancybox/jquery.fancybox.pack.js"></script>


<script src="../../../../js/script.js"></script>

  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>